{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, ElasticNet, ElasticNetCV, Lasso, LogisticRegressionCV, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, f1_score\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAREER_LENGTH = 5\n",
    "\n",
    "#EARLY_CAREER_LEN_LIST = [1, 2, 3, 4, 5]\n",
    "EARLY_CAREER_LEN_LIST = [3]\n",
    "EARLY_CAREER = 3\n",
    "#RECOGNITION_CUT_OFF_LIST = [3, 4, 5, 6, 7, 8, 9]\n",
    "RECOGNITION_CUT_OFF_LIST = [3]\n",
    "RECOGNITION_CUT = 3\n",
    "\n",
    "MAX_CAREER_LEN = 15\n",
    "END_YEAR = 2018\n",
    "\n",
    "INCLUDE_PROD = 1\n",
    "INCLUDE_SOCIAL = 1\n",
    "INCLUDE_REC = 0\n",
    "INCLUDE_GENDER = 1\n",
    "INCLUDE_QUALITY = 1\n",
    "\n",
    "# if true, all authors with gender=none are removed from cohort\n",
    "REMOVE_NONE_AUTHORS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_authors = pd.read_csv('derived-data/authors-scientific-extended.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['author', 'start_year', 'end_year', 'total_num_pub', 'career_length',\n",
       "       'max_absence-0-15', 'avg_absence-0-15', 'dropped_after_10', 'gender',\n",
       "       'early_career_degree_3', 'early_career_degree_5',\n",
       "       'early_career_degree_7', 'early_career_degree_9',\n",
       "       'early_career_degree_11', 'early_career_degree_12',\n",
       "       'early_career_qual_3', 'early_career_qual_5', 'early_career_qual_7',\n",
       "       'early_career_qual_9', 'early_career_qual_11', 'early_career_qual_12',\n",
       "       'early_career_recognition_EC3_RC3', 'early_career_recognition_EC5_RC5',\n",
       "       'early_career_recognition_EC7_RC7', 'early_career_recognition_EC9_RC9',\n",
       "       'early_career_recognition_EC11_RC11',\n",
       "       'early_career_recognition_EC12_RC12', 'succ_after_15y', 'h-index_3',\n",
       "       'h-index_5', 'h-index_7', 'h-index_9', 'h-index_11', 'h-index_12',\n",
       "       'h-index_15', 'early_career_prod_3', 'early_career_prod_5',\n",
       "       'early_career_prod_7', 'early_career_prod_9', 'early_career_prod_11',\n",
       "       'early_career_prod_12', 'early_career_coauthor_max_hindex_3',\n",
       "       'early_career_coauthor_max_hindex_5',\n",
       "       'early_career_coauthor_max_hindex_7',\n",
       "       'early_career_coauthor_max_hindex_9',\n",
       "       'early_career_coauthor_max_hindex_11',\n",
       "       'early_career_coauthor_max_hindex_12'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "credible_authors.columns\n",
    "#print(credible_authors.groupby(\"start_year\")['dropped_after_10'].agg('sum'))\n",
    "#print(credible_authors.groupby(\"start_year\")['author'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_authors = credible_authors[credible_authors.career_length >= CAREER_LENGTH]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# credible_authors['citation_increase_15_3'] = credible_authors['succ_after_15y'] - credible_authors[\n",
    "#     'early_career_recognition_EC3_RC3']\n",
    "# credible_authors['h_index_increase_15_3'] = credible_authors['h-index_15'] - credible_authors['h-index_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLY_CAREER_LEN_LIST_EXT = [3,5,7,9,11,12]\n",
    "RECOGNITION_CUT_OFF_LIST_EXT = [3,5,7,9,11,12]\n",
    "\n",
    "for year in EARLY_CAREER_LEN_LIST_EXT:\n",
    "    credible_authors[f'citation_increase_15_{year}'] = credible_authors['succ_after_15y'] - credible_authors[\n",
    "        f'early_career_recognition_EC{year}_RC{year}']\n",
    "    credible_authors[f'h_index_increase_15_{year}'] = credible_authors['h-index_15'] - credible_authors[f'h-index_{year}']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cols = ['succ_after_15y', 'h_index_increase_15_3', 'citation_increase_15_3', 'max_absence-0-15', \n",
    "        'early_career_prod_3', 'early_career_degree_3', 'early_career_coauthor_max_hindex_12', \n",
    "        'early_career_recognition_EC3_RC3', 'early_career_qual_12']\n",
    "\n",
    "col_names_short = ['succ', 'hindex_incr', 'cit_incr', 'max_abs', \n",
    "        'prod_3', 'degree_3', 'maxh_3', \n",
    "        'rec_3', 'qual_3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cor_qual = credible_authors[cols].corr(method='kendall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cor_qual\n",
    "#cor_qual['succ_after_15y'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cor_qual, vmin=-1, vmax=1)\n",
    "fig.colorbar(cax)\n",
    "ticks = np.arange(0,9,1)\n",
    "ax.set_xticks(ticks)\n",
    "ax.set_yticks(ticks)\n",
    "ax.set_xticklabels(col_names_short, rotation=45)\n",
    "ax.set_yticklabels(col_names_short)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#cor = credible_authors.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#cor_qual[f'h_index_increase_15_{EARLY_CAREER}'].sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# sns.heatmap(cor, center=0,\n",
    "#             square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear reg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Test different predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# test different early career lenghts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "year = 1995\n",
    "\n",
    "credible_authors_1991 = credible_authors[credible_authors.start_year == year]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "X = credible_authors_1991.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "categorical_cols = ['gender']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    X = X.join(pd.get_dummies(X[categorical_cols]))\n",
    "\n",
    "X.drop(categorical_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def run_linear(func, name):\n",
    "    df = pd.DataFrame(columns=['params', f'r_squared_{name}'])\n",
    "    for EARLY_CAREER in EARLY_CAREER_LEN_LIST:\n",
    "        for RECOGNITION_CUT in RECOGNITION_CUT_OFF_LIST:\n",
    "            if RECOGNITION_CUT < EARLY_CAREER: continue\n",
    "            H = X[[\n",
    "                #'max_absence-0-3', 'avg_absence-0-3',\n",
    "                   'gender_f', 'gender_m', 'gender_none',\n",
    "                   f'early_career_degree_{EARLY_CAREER}', \n",
    "                   f'early_career_prod_{EARLY_CAREER}',\n",
    "                   f'early_career_qual_{EARLY_CAREER}', f'early_career_recognition_EC{EARLY_CAREER}_RC{RECOGNITION_CUT}']]\n",
    "            reg = func.fit(H, y)\n",
    "            df = df.append({'params': f'EC:{EARLY_CAREER},REC:{RECOGNITION_CUT}',\n",
    "                            f'r_squared_{name}': reg.score(H, y)}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def run_cv(func, name, cv, y_col='succ_after_15y'):\n",
    "    df = pd.DataFrame(columns=['params', f'r_squared_{name}'])\n",
    "    for EARLY_CAREER in EARLY_CAREER_LEN_LIST:\n",
    "        for RECOGNITION_CUT in RECOGNITION_CUT_OFF_LIST:\n",
    "            if RECOGNITION_CUT < EARLY_CAREER: continue\n",
    "            H = X[[\n",
    "                #'max_absence-0-3', 'avg_absence-0-3',\n",
    "                   'gender_f', 'gender_m', 'gender_none',\n",
    "                   f'early_career_degree_{EARLY_CAREER}', \n",
    "                   f'early_career_prod_{EARLY_CAREER}',\n",
    "                   f'early_career_qual_{EARLY_CAREER}', f'early_career_recognition_EC{EARLY_CAREER}_RC{RECOGNITION_CUT}']]\n",
    "            y = X[y_col]\n",
    "            score = np.mean(cross_val_score(func, H, y, cv=cv, scoring='r2'))\n",
    "            df = df.append({'params': f'EC:{EARLY_CAREER},REC:{RECOGNITION_CUT}',\n",
    "                            f'r_squared_{name}': score}, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df1 = run_cv(LinearRegression(), 'linear', cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df1_null = run_cv(LinearRegression(), 'linear_null', cv=3, y_col='succ_shuffled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df2 = run_cv(ElasticNet(), 'elastic', cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df3 = run_cv(ElasticNetCV(cv=3), 'elastic_CV', cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df4 = run_cv(Lasso(alpha=0.1), 'lasso', cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Decision tree overfits pretty bad. Maybe GridParam Search?\n",
    "df5 = run_cv(DecisionTreeRegressor(), 'tree', cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df6 = run_cv(RandomForestRegressor(), 'forest', cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# df6_null = run_cv(RandomForestRegressor(), 'forest_null', cv=3, y_col='succ_shuffled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfs = [df1, df2, df3, df4, df5, df6] #df1_null, df6_null\n",
    "for df_ in dfs: df_.set_index('params', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dfs[0].join(dfs[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = sorted(credible_authors.start_year.unique())\n",
    "cohort_start_years = [y for y in years if y < (END_YEAR - MAX_CAREER_LEN)]\n",
    "# EARLY_CAREER = EARLY_CAREER_LEN_LIST[0]\n",
    "# RECOGNITION_CUT = RECOGNITION_CUT_OFF_LIST[0]\n",
    "EARLY_CAREER_LEN_LIST = [3,5,7,9,11,12]\n",
    "RECOGNITION_CUT_OFF_LIST = [3,5,7,9,11,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler,RobustScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4 groups of features: productivity, social capital, quality/rec and gender\n",
    "def make_cols_lists(INCLUDE_PROD, INCLUDE_SOCIAL, INCLUDE_REC, INCLUDE_QUALITY, INCLUDE_GENDER, REMOVE_NONE_AUTHORS, \n",
    "                    EARLY_CAREER, RECOGNITION_CUT):\n",
    "    cols_std = []\n",
    "    cols_all = ['cohort_size']\n",
    "    \n",
    "    if(INCLUDE_PROD):\n",
    "        cols_std.append(f'early_career_prod_{EARLY_CAREER}')\n",
    "        cols_all.append(f'early_career_prod_{EARLY_CAREER}')\n",
    "\n",
    "    if(INCLUDE_SOCIAL):\n",
    "        cols_std.append(f'early_career_degree_{EARLY_CAREER}')\n",
    "        cols_all.append(f'early_career_degree_{EARLY_CAREER}')\n",
    "        cols_std.append(f'early_career_coauthor_max_hindex_{EARLY_CAREER}')\n",
    "        cols_all.append(f'early_career_coauthor_max_hindex_{EARLY_CAREER}')\n",
    "    #     cols_std.append(f'early_career_coauthor_max_cit_{EARLY_CAREER}')\n",
    "    #     cols_all.append(f'early_career_coauthor_max_cit_{EARLY_CAREER}')\n",
    "\n",
    "    if(INCLUDE_REC):\n",
    "        cols_std.append(f'early_career_recognition_EC{EARLY_CAREER}_RC{RECOGNITION_CUT}')\n",
    "        cols_all.append(f'early_career_recognition_EC{EARLY_CAREER}_RC{RECOGNITION_CUT}')\n",
    "\n",
    "    if(INCLUDE_QUALITY):\n",
    "        cols_std.append(  f'early_career_qual_{EARLY_CAREER}')\n",
    "        cols_all.append(  f'early_career_qual_{EARLY_CAREER}')\n",
    "\n",
    "    if(INCLUDE_GENDER):\n",
    "        cols_all.append('gender_m')\n",
    "        cols_all.append('gender_f')\n",
    "        if(not REMOVE_NONE_AUTHORS):\n",
    "            cols_all.append('gender_none')\n",
    "\n",
    "\n",
    "    cols_all.append('intercept')\n",
    "    cols_all.append('r2')\n",
    "    categorical_cols = ['gender']\n",
    "    return cols_all, cols_std, categorical_cols\n",
    "num_splits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dep_var = f'h_index_increase_15_{EARLY_CAREER}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1970</th>\n",
       "      <th>1971</th>\n",
       "      <th>1972</th>\n",
       "      <th>1973</th>\n",
       "      <th>1974</th>\n",
       "      <th>1975</th>\n",
       "      <th>1976</th>\n",
       "      <th>1977</th>\n",
       "      <th>1978</th>\n",
       "      <th>1979</th>\n",
       "      <th>...</th>\n",
       "      <th>1993</th>\n",
       "      <th>1994</th>\n",
       "      <th>1995</th>\n",
       "      <th>1996</th>\n",
       "      <th>1997</th>\n",
       "      <th>1998</th>\n",
       "      <th>1999</th>\n",
       "      <th>2000</th>\n",
       "      <th>2001</th>\n",
       "      <th>2002</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>feature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cohort_size</th>\n",
       "      <td>390</td>\n",
       "      <td>508</td>\n",
       "      <td>609</td>\n",
       "      <td>767</td>\n",
       "      <td>847</td>\n",
       "      <td>813</td>\n",
       "      <td>993</td>\n",
       "      <td>1073</td>\n",
       "      <td>1039</td>\n",
       "      <td>1291</td>\n",
       "      <td>...</td>\n",
       "      <td>8899</td>\n",
       "      <td>9814</td>\n",
       "      <td>10019</td>\n",
       "      <td>10893</td>\n",
       "      <td>11872</td>\n",
       "      <td>13594</td>\n",
       "      <td>14939</td>\n",
       "      <td>17593</td>\n",
       "      <td>19195</td>\n",
       "      <td>21220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early_career_prod_3</th>\n",
       "      <td>0.37(0.05)</td>\n",
       "      <td>0.45(0.06)</td>\n",
       "      <td>0.52(0.05)</td>\n",
       "      <td>0.51(0.04)</td>\n",
       "      <td>0.48(0.03)</td>\n",
       "      <td>0.49(0.03)</td>\n",
       "      <td>0.61(0.05)</td>\n",
       "      <td>0.41(0.11)</td>\n",
       "      <td>0.57(0.04)</td>\n",
       "      <td>0.55(0.04)</td>\n",
       "      <td>...</td>\n",
       "      <td>1.15(0.01)</td>\n",
       "      <td>1.26(0.05)</td>\n",
       "      <td>1.21(0.04)</td>\n",
       "      <td>1.14(0.02)</td>\n",
       "      <td>1.15(0.03)</td>\n",
       "      <td>1.19(0.03)</td>\n",
       "      <td>1.16(0.03)</td>\n",
       "      <td>1.2(0.01)</td>\n",
       "      <td>1.12(0.01)</td>\n",
       "      <td>1.04(0.01)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early_career_degree_3</th>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>-0.0(0.02)</td>\n",
       "      <td>-0.0(0.01)</td>\n",
       "      <td>0.04(0.02)</td>\n",
       "      <td>0.06(0.01)</td>\n",
       "      <td>0.03(0.02)</td>\n",
       "      <td>0.04(0.03)</td>\n",
       "      <td>0.05(0.06)</td>\n",
       "      <td>-0.02(0.05)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.03(0.01)</td>\n",
       "      <td>-0.04(0.01)</td>\n",
       "      <td>-0.1(0.02)</td>\n",
       "      <td>-0.03(0.01)</td>\n",
       "      <td>-0.04(0.01)</td>\n",
       "      <td>-0.04(0.03)</td>\n",
       "      <td>-0.03(0.02)</td>\n",
       "      <td>-0.02(0.01)</td>\n",
       "      <td>-0.05(0.01)</td>\n",
       "      <td>-0.0(0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early_career_coauthor_max_hindex_3</th>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.48(0.12)</td>\n",
       "      <td>0.08(0.19)</td>\n",
       "      <td>0.47(0.08)</td>\n",
       "      <td>0.26(0.04)</td>\n",
       "      <td>0.18(0.08)</td>\n",
       "      <td>0.13(0.05)</td>\n",
       "      <td>0.39(0.03)</td>\n",
       "      <td>0.26(0.05)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.48(0.02)</td>\n",
       "      <td>0.46(0.02)</td>\n",
       "      <td>0.46(0.02)</td>\n",
       "      <td>0.46(0.01)</td>\n",
       "      <td>0.59(0.02)</td>\n",
       "      <td>0.61(0.04)</td>\n",
       "      <td>0.59(0.03)</td>\n",
       "      <td>0.63(0.03)</td>\n",
       "      <td>0.81(0.02)</td>\n",
       "      <td>0.71(0.03)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early_career_recognition_EC3_RC3</th>\n",
       "      <td>0.03(0.03)</td>\n",
       "      <td>0.03(0.01)</td>\n",
       "      <td>-0.03(0.03)</td>\n",
       "      <td>0.07(0.02)</td>\n",
       "      <td>0.03(0.01)</td>\n",
       "      <td>0.03(0.03)</td>\n",
       "      <td>0.05(0.02)</td>\n",
       "      <td>0.08(0.01)</td>\n",
       "      <td>0.01(0.02)</td>\n",
       "      <td>0.02(0.01)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11(0.01)</td>\n",
       "      <td>0.08(0.02)</td>\n",
       "      <td>0.09(0.02)</td>\n",
       "      <td>0.1(0.02)</td>\n",
       "      <td>0.09(0.01)</td>\n",
       "      <td>0.08(0.01)</td>\n",
       "      <td>0.11(0.01)</td>\n",
       "      <td>0.11(0.01)</td>\n",
       "      <td>0.07(0.01)</td>\n",
       "      <td>0.07(0.01)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender_m</th>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.0(0.01)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.17(0.1)</td>\n",
       "      <td>0.07(0.08)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.07(0.15)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06(0.03)</td>\n",
       "      <td>0.02(0.03)</td>\n",
       "      <td>0.07(0.05)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.03(0.02)</td>\n",
       "      <td>-0.01(0.02)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender_f</th>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.01(0.01)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>-0.01(0.02)</td>\n",
       "      <td>-0.06(0.13)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0(0.0)</td>\n",
       "      <td>0.01(0.01)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.02(0.04)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>-0.02(0.03)</td>\n",
       "      <td>-0.15(0.03)</td>\n",
       "      <td>-0.13(0.03)</td>\n",
       "      <td>-0.16(0.08)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender_none</th>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>-0.01(0.01)</td>\n",
       "      <td>-0.0(0.0)</td>\n",
       "      <td>-0.0(0.0)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>-0.02(0.05)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>0.01(0.03)</td>\n",
       "      <td>0.0(0.0)</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.0(0.0)</td>\n",
       "      <td>-0.16(0.05)</td>\n",
       "      <td>-0.07(0.05)</td>\n",
       "      <td>-0.02(0.04)</td>\n",
       "      <td>-0.03(0.05)</td>\n",
       "      <td>0.02(0.02)</td>\n",
       "      <td>0.07(0.05)</td>\n",
       "      <td>0.14(0.06)</td>\n",
       "      <td>0.12(0.05)</td>\n",
       "      <td>0.07(0.05)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.86</td>\n",
       "      <td>1.16</td>\n",
       "      <td>0.9</td>\n",
       "      <td>1.19</td>\n",
       "      <td>...</td>\n",
       "      <td>2.02</td>\n",
       "      <td>2.38</td>\n",
       "      <td>2.33</td>\n",
       "      <td>2.34</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.16</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          1970        1971         1972  \\\n",
       "feature                                                                   \n",
       "cohort_size                                390         508          609   \n",
       "early_career_prod_3                 0.37(0.05)  0.45(0.06)   0.52(0.05)   \n",
       "early_career_degree_3                 0.0(0.0)  -0.0(0.02)   -0.0(0.01)   \n",
       "early_career_coauthor_max_hindex_3    0.0(0.0)    0.0(0.0)   0.48(0.12)   \n",
       "early_career_recognition_EC3_RC3    0.03(0.03)  0.03(0.01)  -0.03(0.03)   \n",
       "gender_m                              0.0(0.0)    0.0(0.0)    0.0(0.01)   \n",
       "gender_f                              0.0(0.0)    0.0(0.0)     0.0(0.0)   \n",
       "gender_none                           0.0(0.0)    0.0(0.0)  -0.01(0.01)   \n",
       "intercept                                 0.93        0.81         0.87   \n",
       "r2                                        0.16        0.14         0.21   \n",
       "\n",
       "                                          1973        1974        1975  \\\n",
       "feature                                                                  \n",
       "cohort_size                                767         847         813   \n",
       "early_career_prod_3                 0.51(0.04)  0.48(0.03)  0.49(0.03)   \n",
       "early_career_degree_3               0.04(0.02)  0.06(0.01)  0.03(0.02)   \n",
       "early_career_coauthor_max_hindex_3  0.08(0.19)  0.47(0.08)  0.26(0.04)   \n",
       "early_career_recognition_EC3_RC3    0.07(0.02)  0.03(0.01)  0.03(0.03)   \n",
       "gender_m                              0.0(0.0)   0.17(0.1)  0.07(0.08)   \n",
       "gender_f                            0.01(0.01)    0.0(0.0)    0.0(0.0)   \n",
       "gender_none                          -0.0(0.0)   -0.0(0.0)    0.0(0.0)   \n",
       "intercept                                 0.83        0.73        0.77   \n",
       "r2                                        0.22        0.25        0.24   \n",
       "\n",
       "                                           1976         1977         1978  \\\n",
       "feature                                                                     \n",
       "cohort_size                                 993         1073         1039   \n",
       "early_career_prod_3                  0.61(0.05)   0.41(0.11)   0.57(0.04)   \n",
       "early_career_degree_3                0.04(0.03)   0.05(0.06)  -0.02(0.05)   \n",
       "early_career_coauthor_max_hindex_3   0.18(0.08)   0.13(0.05)   0.39(0.03)   \n",
       "early_career_recognition_EC3_RC3     0.05(0.02)   0.08(0.01)   0.01(0.02)   \n",
       "gender_m                               0.0(0.0)   0.07(0.15)     0.0(0.0)   \n",
       "gender_f                               0.0(0.0)  -0.01(0.02)  -0.06(0.13)   \n",
       "gender_none                         -0.02(0.05)     0.0(0.0)   0.01(0.03)   \n",
       "intercept                                  0.86         1.16          0.9   \n",
       "r2                                          0.2          0.1         0.23   \n",
       "\n",
       "                                          1979     ...              1993  \\\n",
       "feature                                            ...                     \n",
       "cohort_size                               1291     ...              8899   \n",
       "early_career_prod_3                 0.55(0.04)     ...        1.15(0.01)   \n",
       "early_career_degree_3                 0.0(0.0)     ...       -0.03(0.01)   \n",
       "early_career_coauthor_max_hindex_3  0.26(0.05)     ...        0.48(0.02)   \n",
       "early_career_recognition_EC3_RC3    0.02(0.01)     ...        0.11(0.01)   \n",
       "gender_m                              0.0(0.0)     ...        0.06(0.03)   \n",
       "gender_f                              0.0(0.0)     ...         -0.0(0.0)   \n",
       "gender_none                           0.0(0.0)     ...         -0.0(0.0)   \n",
       "intercept                                 1.19     ...              2.02   \n",
       "r2                                        0.17     ...              0.23   \n",
       "\n",
       "                                           1994         1995         1996  \\\n",
       "feature                                                                     \n",
       "cohort_size                                9814        10019        10893   \n",
       "early_career_prod_3                  1.26(0.05)   1.21(0.04)   1.14(0.02)   \n",
       "early_career_degree_3               -0.04(0.01)   -0.1(0.02)  -0.03(0.01)   \n",
       "early_career_coauthor_max_hindex_3   0.46(0.02)   0.46(0.02)   0.46(0.01)   \n",
       "early_career_recognition_EC3_RC3     0.08(0.02)   0.09(0.02)    0.1(0.02)   \n",
       "gender_m                             0.02(0.03)   0.07(0.05)     0.0(0.0)   \n",
       "gender_f                             0.01(0.01)     0.0(0.0)   0.02(0.04)   \n",
       "gender_none                         -0.16(0.05)  -0.07(0.05)  -0.02(0.04)   \n",
       "intercept                                  2.38         2.33         2.34   \n",
       "r2                                         0.22         0.21         0.23   \n",
       "\n",
       "                                           1997         1998         1999  \\\n",
       "feature                                                                     \n",
       "cohort_size                               11872        13594        14939   \n",
       "early_career_prod_3                  1.15(0.03)   1.19(0.03)   1.16(0.03)   \n",
       "early_career_degree_3               -0.04(0.01)  -0.04(0.03)  -0.03(0.02)   \n",
       "early_career_coauthor_max_hindex_3   0.59(0.02)   0.61(0.04)   0.59(0.03)   \n",
       "early_career_recognition_EC3_RC3     0.09(0.01)   0.08(0.01)   0.11(0.01)   \n",
       "gender_m                             0.03(0.02)  -0.01(0.02)     0.0(0.0)   \n",
       "gender_f                               0.0(0.0)     0.0(0.0)  -0.02(0.03)   \n",
       "gender_none                         -0.03(0.05)   0.02(0.02)   0.07(0.05)   \n",
       "intercept                                  2.42         2.46         2.48   \n",
       "r2                                         0.22         0.23         0.23   \n",
       "\n",
       "                                           2000         2001         2002  \n",
       "feature                                                                    \n",
       "cohort_size                               17593        19195        21220  \n",
       "early_career_prod_3                   1.2(0.01)   1.12(0.01)   1.04(0.01)  \n",
       "early_career_degree_3               -0.02(0.01)  -0.05(0.01)    -0.0(0.0)  \n",
       "early_career_coauthor_max_hindex_3   0.63(0.03)   0.81(0.02)   0.71(0.03)  \n",
       "early_career_recognition_EC3_RC3     0.11(0.01)   0.07(0.01)   0.07(0.01)  \n",
       "gender_m                               0.0(0.0)     0.0(0.0)     0.0(0.0)  \n",
       "gender_f                            -0.15(0.03)  -0.13(0.03)  -0.16(0.08)  \n",
       "gender_none                          0.14(0.06)   0.12(0.05)   0.07(0.05)  \n",
       "intercept                                   2.5          2.5         2.58  \n",
       "r2                                         0.27         0.25         0.26  \n",
       "\n",
       "[10 rows x 33 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INCLUDE_PROD = 1\n",
    "INCLUDE_SOCIAL = 1\n",
    "INCLUDE_REC = 1\n",
    "INCLUDE_GENDER = 1\n",
    "INCLUDE_QUALITY = 0\n",
    "REMOVE_NONE_AUTHORS = 0\n",
    "INCLUDE_YEAR = 0\n",
    "cols_all, cols_std, categorical_cols = make_cols_lists(INCLUDE_PROD, INCLUDE_SOCIAL, INCLUDE_REC, INCLUDE_QUALITY, \n",
    "                                                       INCLUDE_GENDER, REMOVE_NONE_AUTHORS, EARLY_CAREER, RECOGNITION_CUT)\n",
    "run_elastic_net_cohort(credible_authors,cols_all, cols_std, categorical_cols, EARLY_CAREER, dep_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>early_career_prod_3</th>\n",
       "      <td>0.25(0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early_career_degree_3</th>\n",
       "      <td>-0.01(0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early_career_coauthor_max_hindex_3</th>\n",
       "      <td>0.02(0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early_career_recognition_EC3_RC3</th>\n",
       "      <td>-0.02(0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>early_career_qual_3</th>\n",
       "      <td>0.01(0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender_f</th>\n",
       "      <td>0.0(0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender_m</th>\n",
       "      <td>0.0(0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender_none</th>\n",
       "      <td>0.0(0.0)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_year</th>\n",
       "      <td>-0.61(0.01)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>intercept</th>\n",
       "      <td>1.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r2</th>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cohort_size</th>\n",
       "      <td>503859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              1\n",
       "0                                              \n",
       "early_career_prod_3                   0.25(0.0)\n",
       "early_career_degree_3                -0.01(0.0)\n",
       "early_career_coauthor_max_hindex_3    0.02(0.0)\n",
       "early_career_recognition_EC3_RC3     -0.02(0.0)\n",
       "early_career_qual_3                   0.01(0.0)\n",
       "gender_f                               0.0(0.0)\n",
       "gender_m                               0.0(0.0)\n",
       "gender_none                            0.0(0.0)\n",
       "start_year                          -0.61(0.01)\n",
       "intercept                                  1.49\n",
       "r2                                         0.13\n",
       "cohort_size                              503859"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INCLUDE_PROD = 1\n",
    "INCLUDE_SOCIAL = 1\n",
    "INCLUDE_REC = 1\n",
    "INCLUDE_GENDER = 1\n",
    "INCLUDE_QUALITY = 1\n",
    "REMOVE_NONE_AUTHORS = 0\n",
    "INCLUDE_YEAR = 1\n",
    "cols_all, cols_std, categorical_cols = make_cols_lists(INCLUDE_PROD, INCLUDE_SOCIAL, INCLUDE_REC, INCLUDE_QUALITY, \n",
    "                                                       INCLUDE_GENDER, REMOVE_NONE_AUTHORS, EARLY_CAREER, RECOGNITION_CUT)\n",
    "run_elastic_net_aggr(credible_authors,cols_all, cols_std, categorical_cols, EARLY_CAREER, dep_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def scale_columns(X):\n",
    "    if len(X.columns) > 0:\n",
    "        standardized_cols = RobustScaler().fit_transform(X)\n",
    "    else: \n",
    "        standardized_cols = []\n",
    "    return pd.DataFrame(standardized_cols, index=X.index, columns=X.columns)\n",
    "\n",
    "def prepare_data(credible_authors):\n",
    "    X = credible_authors.copy()\n",
    "    for year in cohort_start_years:\n",
    "        X.loc[X.start_year == year, cols_std] = scale_columns(X.loc[X.start_year == year, cols_std])\n",
    "    cat_cols = pd.get_dummies(X[categorical_cols]) \n",
    "    X = X[cols_std].join(cat_cols)\n",
    "    X['start_year'] = credible_authors['start_year']\n",
    "    return X\n",
    "def run_elastic_net_aggr(credible_authors,cols_all, cols_std, categorical_cols, EARLY_CAREER, dep_var):\n",
    "    X = prepare_data(credible_authors)\n",
    "    Y = credible_authors[dep_var]\n",
    "    X['start_year'] = RobustScaler().fit_transform(X['start_year'].to_frame())\n",
    "    \n",
    "    feat_table = run_elastic_net(X, Y)\n",
    "    feat_table = feat_table.set_index(0)\n",
    "    return feat_table\n",
    "\n",
    "def run_elastic_net_cohort(credible_authors,cols_all, cols_std, categorical_cols, EARLY_CAREER, dep_var):\n",
    "    feat_table = pd.DataFrame(cols_all, columns=['feature']).set_index('feature')\n",
    "    X = prepare_data(credible_authors)\n",
    "    for year in cohort_start_years:\n",
    "        X_year = X[X.start_year == year]\n",
    "        y_year = credible_authors[credible_authors.start_year == year][dep_var]\n",
    "        \n",
    "        feat_data = run_elastic_net(X_year.drop('start_year', axis=1), y_year)\n",
    "        feat_data = feat_data.set_index(0)\n",
    "        feat_data.rename(index=str, columns={1: year}, inplace=True)\n",
    "       \n",
    "        feat_table = feat_table.join(feat_data)\n",
    "        \n",
    "    return feat_table\n",
    "\n",
    "def run_elastic_net(X, y):\n",
    "    # train model and do cross validation\n",
    "    cv_dict = cross_validate(ElasticNetCV(cv=3), X, y, scoring='r2', cv=5, return_estimator=True, return_train_score=False)\n",
    "          \n",
    "    score = np.mean(cv_dict['test_score'])\n",
    "    # save the coefficients and intercepts\n",
    "    net_coef = pd.DataFrame([es.coef_ for es in cv_dict['estimator']], columns=X.columns)\n",
    "    net_intercept = np.mean([es.intercept_ for es in cv_dict['estimator']])\n",
    "    # take the mean and std from coefs\n",
    "    net_coef_mean = net_coef.mean()\n",
    "    net_coef_std = net_coef.std()\n",
    "    rounding = 2\n",
    "    net_coef_mean_std = list(zip(np.round(net_coef_mean.values,rounding), np.round(net_coef_std.values,rounding)))\n",
    "    net_coef_mean_std = [f\"{x[0]}({x[1]})\" for x in net_coef_mean_std]\n",
    "\n",
    "    cohort_size = len(y)\n",
    "    #     num_nonzero_coefs = sum(net2.coef_ != 0)\n",
    "    #     adj_score2 = 1 - (1-score2)*(cohort_size-1)/(cohort_size-num_nonzero_coefs-1)\n",
    "    net_coef_mean_std.extend([np.round(net_intercept, rounding), np.round(score, rounding), cohort_size])\n",
    "    feat_table = pd.DataFrame(list(zip(np.append(X.columns, ['intercept', 'r2', 'cohort_size']), net_coef_mean_std)))\n",
    "    \n",
    "    return feat_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     17,
     29
    ]
   },
   "outputs": [],
   "source": [
    "def run_elastic_net_cohort(credible_authors,cols_all, cols_std, categorical_cols, EARLY_CAREER):\n",
    "    aggregated = False\n",
    "    hindex_table = pd.DataFrame(cols_all, columns=['feature']).set_index('feature')\n",
    "    citation_table = pd.DataFrame(cols_all, columns=['feature']).set_index('feature')\n",
    "    dropout_table = pd.DataFrame(cols_all, columns=['feature']).set_index('feature')\n",
    "    for year in cohort_start_years:\n",
    "        credible_authors_year = credible_authors[credible_authors.start_year == year]\n",
    "        X = credible_authors_year.copy()\n",
    "        \n",
    "        hindex_data, citation_data, dropout_data = run_elastic_net(X, cols_std, categorical_cols, EARLY_CAREER, year, aggregated)\n",
    "       \n",
    "        hindex_table = hindex_table.join(hindex_data)\n",
    "        citation_table = citation_table.join(citation_data)\n",
    "        dropout_table = dropout_table.join(dropout_data)\n",
    "        \n",
    "    return hindex_table, citation_table, dropout_table\n",
    "\n",
    "def run_elastic_net_aggr(credible_authors, cols_std, categorical_cols, EARLY_CAREER):\n",
    "        \n",
    "    aggregated = True\n",
    "    #add cohort start year as var\n",
    "    categorical_cols.append(\"start_year\")\n",
    "    \n",
    "    X = credible_authors.copy()\n",
    "    \n",
    "    return run_elastic_net(X, cols_std, categorical_cols, EARLY_CAREER, year=0, aggregated)\n",
    "    \n",
    "\n",
    "\n",
    "def run_elastic_net(X, cols_std, categorical_cols, EARLY_CAREER, year=0, aggregated):\n",
    "   \n",
    "    #remove non-gender rows\n",
    "    if(REMOVE_NONE_AUTHORS):\n",
    "        X = X[X[\"gender\"]!=\"none\"]\n",
    "\n",
    "   \n",
    "    # Make dummy values for categorical columns\n",
    "    cat_cols = pd.get_dummies(X[categorical_cols])\n",
    "\n",
    "    \n",
    "    #if(not REMOVE_NONE_AUTHORS):\n",
    "        # drop gender none?\n",
    "         # this is removing rows gender_none col\n",
    "        #gender_cols.drop('gender_none', axis=1, inplace=True)\n",
    "\n",
    "    #standardize cols_std\n",
    "    if len(cols_std)>0:\n",
    "        standardized_cols = RobustScaler().fit_transform(X[cols_std])\n",
    "        H = pd.DataFrame(standardized_cols, index=X.index, columns=cols_std)\n",
    "    else:\n",
    "        H = pd.DataFrame(index=X.index, columns=cols_std)\n",
    "       \n",
    "    H = H.join(cat_cols) \n",
    "    if(not INCLUDE_GENDER):\n",
    "        H = H.drop(columns=['gender_f',  'gender_m',  'gender_none'])\n",
    "    \n",
    "    print(H.head())\n",
    "    \n",
    "    y = X[f'h_index_increase_15_{EARLY_CAREER}']\n",
    "    y2 = X[f'citation_increase_15_{EARLY_CAREER}']\n",
    "    y3 = X['dropped_after_10'].astype(int)\n",
    "\n",
    "    #cv_dict = cross_validate(LinearRegressionCV(cv=3), H, y, scoring='r2', cv=10, return_estimator=True, return_train_score=False)\n",
    "    #cv2_dict = cross_validate(LinearRegressionCV(cv=3), H, y2, scoring='r2', cv=10, return_estimator=True, return_train_score=False)\n",
    "    #cv3_dict = cross_validate(LogisticRegressionCV(cv=3, penalty='l2'), H, y3, scoring=\"f1\", cv=10, return_estimator=True, return_train_score=False)\n",
    "    cv_dict = cross_validate(ElasticNetCV(cv=3), H, y, scoring='r2', cv=10, return_estimator=True, return_train_score=False)\n",
    "    cv2_dict = cross_validate(ElasticNetCV(cv=3), H, y2, scoring='r2', cv=10, return_estimator=True, return_train_score=False)\n",
    "    cv3_dict = cross_validate(LogisticRegressionCV(cv=3, penalty='l2'), H, y3, scoring=\"f1\", cv=10, return_estimator=True, return_train_score=False)\n",
    "          \n",
    "    score = np.mean(cv_dict['test_score'])\n",
    "    score2 = np.mean(cv2_dict['test_score'])\n",
    "    score3 = np.mean(cv3_dict['test_score'])\n",
    "\n",
    "    net_coef = pd.DataFrame([es.coef_ for es in cv_dict['estimator']], columns=H.columns)\n",
    "    net2_coef = pd.DataFrame([es.coef_ for es in cv2_dict['estimator']], columns=H.columns)\n",
    "    net3_coef = pd.DataFrame([es.coef_[0] for es in cv3_dict['estimator']], columns=H.columns)\n",
    "\n",
    "    net_intercept = np.mean([es.intercept_ for es in cv_dict['estimator']])\n",
    "    net2_intercept = np.mean([es.intercept_ for es in cv2_dict['estimator']])\n",
    "    net3_intercept = np.mean([es.intercept_ for es in cv3_dict['estimator']])\n",
    "\n",
    "    net_coef_mean, net2_coef_mean, net3_coef_mean = net_coef.mean(), net2_coef.mean(), net3_coef.mean()\n",
    "    net_coef_std, net2_coef_std, net3_coef_std = net_coef.std(), net2_coef.std(), net3_coef.std()\n",
    "\n",
    "    rounding = 2\n",
    "\n",
    "    net_coef_mean_std = list(zip(np.round(net_coef_mean.values,rounding), np.round(net_coef_std.values,rounding)))\n",
    "    net2_coef_mean_std = list(zip(np.round(net2_coef_mean.values,rounding), np.round(net2_coef_std.values,rounding)))\n",
    "    net3_coef_mean_std = list(zip(np.round(net3_coef_mean.values,rounding), np.round(net3_coef_std.values,rounding)))\n",
    "        \n",
    "    net_coef_mean_std = [f\"{x[0]}({x[1]})\" for x in net_coef_mean_std]\n",
    "    net2_coef_mean_std = [f\"{x[0]}({x[1]})\" for x in net2_coef_mean_std]\n",
    "    net3_coef_mean_std = [f\"{x[0]}({x[1]})\" for x in net3_coef_mean_std]\n",
    "\n",
    "    cohort_size = len(y2)\n",
    "    #     num_nonzero_coefs = sum(net2.coef_ != 0)\n",
    "    #     adj_score2 = 1 - (1-score2)*(cohort_size-1)/(cohort_size-num_nonzero_coefs-1)\n",
    "    net_coef_mean_std.extend([np.round(net_intercept, rounding), np.round(score, rounding), cohort_size])\n",
    "    net2_coef_mean_std.extend([np.round(net2_intercept, rounding), np.round(score2, rounding), cohort_size])\n",
    "    net3_coef_mean_std.extend([np.round(net3_intercept, rounding), np.round(score3, rounding), cohort_size])\n",
    "\n",
    "    if(year>0):\n",
    "        hindex_data = pd.DataFrame(list(zip(np.append(H.columns, ['intercept', 'r2', 'cohort_size']), \n",
    "                                              net_coef_mean_std)), \n",
    "                                     columns=['year', year]).set_index('year')\n",
    "        citation_data = pd.DataFrame(list(zip(np.append(H.columns, ['intercept', 'r2', 'cohort_size']), \n",
    "                                              net2_coef_mean_std)), \n",
    "                                     columns=['year', year]).set_index('year')\n",
    "        dropout_data = pd.DataFrame(list(zip(np.append(H.columns, ['intercept', 'r2', 'cohort_size']), \n",
    "                                              net3_coef_mean_std)),\n",
    "                                     columns=['year', year]).set_index('year')\n",
    "    else:\n",
    "        hindex_data = pd.DataFrame(list(zip(np.append(H.columns, ['intercept', 'r2', 'cohort_size']), \n",
    "                                              net_coef_mean_std)))\n",
    "        citation_data = pd.DataFrame(list(zip(np.append(H.columns, ['intercept', 'r2', 'cohort_size']), \n",
    "                                              net2_coef_mean_std)))                                  \n",
    "        dropout_data = pd.DataFrame(list(zip(np.append(H.columns, ['intercept', 'r2', 'cohort_size']), \n",
    "                                              net3_coef_mean_std)))\n",
    "    \n",
    "    return hindex_data, citation_data, dropout_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_result_table(feature_table):\n",
    "    results = feature_table.transpose()\n",
    "    #shorten column names\n",
    "    new_cols = dict(zip(results.columns, [col.replace('early_career', 'ec') for col in results.columns]))\n",
    "\n",
    "    results.rename(new_cols, axis='columns', inplace=True)\n",
    "    results.rename({'feature':'cohort','ec_coauthor_max_cit_3': 'ec_coauth_max_cit_3', 'ec_recognition_EC3_RC5':'ec_recog_EC3_RC5'}, axis='columns', inplace=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_latex(results, name):\n",
    "    ltx_file = open(f\"results_{name}.tex\", \"w\")\n",
    "    ltx_file.write('\\n'.join(results.to_latex().split('\\n')[4:-3]))\n",
    "    ltx_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLY_CAREER = 3\n",
    "RECOGNITION_CUT = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregated Elastic Net Models\n",
    "First we test the effect of different groups of features (human capital, social capital and gender) on success/dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 1: null model\n",
    "\n",
    "INCLUDE_PROD = 0\n",
    "INCLUDE_SOCIAL = 0\n",
    "INCLUDE_REC = 0\n",
    "INCLUDE_GENDER = 0\n",
    "INCLUDE_QUALITY = 0\n",
    "cols_all, cols_std, categorical_cols = make_cols_lists(INCLUDE_PROD, INCLUDE_SOCIAL, INCLUDE_REC, INCLUDE_QUALITY, \n",
    "                                                       INCLUDE_GENDER, REMOVE_NONE_AUTHORS, EARLY_CAREER, RECOGNITION_CUT)\n",
    "\n",
    "#hindex_table, citation_table, dropout_table =  run_elastic_predictions(cols_all, cols_std, categorical_cols, EARLY_CAREER)\n",
    "\n",
    "\n",
    "hindex_table, citation_table, dropout_table = run_elastic_net_aggr(credible_authors, cols_std, categorical_cols, EARLY_CAREER)\n",
    "\n",
    "\n",
    "\n",
    "#hindex_table, citation_table, dropout_table =  run_elastic_net_cohort(credible_authors, cols_all, cols_std, categorical_cols, EARLY_CAREER)\n",
    "#results_hindex = make_result_table(hindex_table)\n",
    "#results_citation = make_result_table(citation_table)\n",
    "#results_dropouts = make_result_table(dropout_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"hindex_table\")\n",
    "print(hindex_table)\n",
    "print(\"citation_table\")\n",
    "print(citation_table)\n",
    "print(\"dropout_table\")\n",
    "print(dropout_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 2: gender effect model\n",
    "\n",
    "INCLUDE_PROD = 1\n",
    "INCLUDE_SOCIAL = 0\n",
    "INCLUDE_REC = 1\n",
    "INCLUDE_GENDER = 1\n",
    "INCLUDE_QUALITY = 0\n",
    "cols_all, cols_std, categorical_cols = make_cols_lists(INCLUDE_PROD, INCLUDE_SOCIAL, INCLUDE_REC, INCLUDE_QUALITY, \n",
    "                                                       INCLUDE_GENDER, REMOVE_NONE_AUTHORS, EARLY_CAREER, RECOGNITION_CUT)\n",
    "\n",
    "hindex_table, citation_table, dropout_table = run_elastic_net_aggr(credible_authors, cols_std, categorical_cols, EARLY_CAREER)\n",
    "print(\"hindex_table\")\n",
    "print(hindex_table)\n",
    "print(\"citation_table\")\n",
    "print(citation_table)\n",
    "print(\"dropout_table\")\n",
    "print(dropout_table)\n",
    "\n",
    "#feature_table, feature_table2, feature_table3 = run_elastic_net_cohort(cols_all, cols_std, categorical_cols, EARLY_CAREER)\n",
    "#results = make_result_table(feature_table)\n",
    "#results2 = make_result_table(feature_table2)\n",
    "#results3 = make_result_table(feature_table3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 3: social capital effect model\n",
    "\n",
    "INCLUDE_PROD = 1\n",
    "INCLUDE_SOCIAL = 1\n",
    "INCLUDE_REC = 1\n",
    "INCLUDE_GENDER = 1\n",
    "INCLUDE_QUALITY = 0\n",
    "cols_all, cols_std, categorical_cols = make_cols_lists(INCLUDE_PROD, INCLUDE_SOCIAL, INCLUDE_REC, INCLUDE_QUALITY, \n",
    "                                                       INCLUDE_GENDER, REMOVE_NONE_AUTHORS, EARLY_CAREER, RECOGNITION_CUT)\n",
    "\n",
    "#feature_table, feature_table2, feature_table3 = run_elastic_bet_cohort(cols_all, cols_std, categorical_cols, EARLY_CAREER)\n",
    "#results = make_result_table(feature_table)\n",
    "#results2 = make_result_table(feature_table2)\n",
    "#results3 = make_result_table(feature_table3)\n",
    "\n",
    "hindex_table, citation_table, dropout_table = run_elastic_net_aggr(credible_authors, cols_std, categorical_cols, EARLY_CAREER)\n",
    "print(\"hindex_table\")\n",
    "print(hindex_table)\n",
    "print(\"citation_table\")\n",
    "print(citation_table)\n",
    "print(\"dropout_table\")\n",
    "print(dropout_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL 4: full model\n",
    "INCLUDE_PROD = 1\n",
    "INCLUDE_SOCIAL = 1\n",
    "INCLUDE_REC = 1\n",
    "INCLUDE_GENDER = 1\n",
    "INCLUDE_QUALITY = 1\n",
    "cols_all, cols_std, categorical_cols = make_cols_lists(INCLUDE_PROD, INCLUDE_SOCIAL, INCLUDE_REC, INCLUDE_QUALITY, \n",
    "                                                       INCLUDE_GENDER, REMOVE_NONE_AUTHORS, EARLY_CAREER, RECOGNITION_CUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hindex_table, citation_table, dropout_table = run_elastic_net_aggr(credible_authors, cols_std, categorical_cols, EARLY_CAREER)\n",
    "print(\"hindex_table\")\n",
    "print(hindex_table)\n",
    "print(\"citation_table\")\n",
    "print(citation_table)\n",
    "print(\"dropout_table\")\n",
    "print(dropout_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cohort Elastic Net Models\n",
    "Second we compare the predictive performance across cohorts. We should plot R2 and F1 over cohorts.\n",
    "Is predictive performance stable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#full model\n",
    "INCLUDE_PROD = 1\n",
    "INCLUDE_SOCIAL = 1\n",
    "INCLUDE_REC = 1\n",
    "INCLUDE_GENDER = 1\n",
    "INCLUDE_QUALITY = 1\n",
    "cols_all, cols_std, categorical_cols = make_cols_lists(INCLUDE_PROD, INCLUDE_SOCIAL, INCLUDE_REC, INCLUDE_QUALITY, \n",
    "                                                       INCLUDE_GENDER, REMOVE_NONE_AUTHORS, EARLY_CAREER, RECOGNITION_CUT)\n",
    "feature_table, feature_table2, feature_table3 = run_elastic_net_cohort(credible_authors, cols_all, cols_std, categorical_cols, EARLY_CAREER)\n",
    "\n",
    "results = make_result_table(feature_table)\n",
    "results2 = make_result_table(feature_table2)\n",
    "results3 = make_result_table(feature_table3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(results.index, results[[\"r2\"]])\n",
    "ax.set(xlabel='cohorts', ylabel='R2',\n",
    "       title='h-index increase prediction')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(results2.index, results2[[\"r2\"]])\n",
    "ax.set(xlabel='cohorts', ylabel='R2',\n",
    "       title='citation increase prediction')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(results3.index, results3[[\"r2\"]])\n",
    "ax.set(xlabel='cohorts', ylabel='F1',\n",
    "       title='dropout prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model without quality \n",
    "INCLUDE_PROD = 1\n",
    "INCLUDE_SOCIAL = 1\n",
    "INCLUDE_REC = 1\n",
    "INCLUDE_GENDER = 1\n",
    "INCLUDE_QUALITY = 0\n",
    "cols_all, cols_std, categorical_cols = make_cols_lists(INCLUDE_PROD, INCLUDE_SOCIAL, INCLUDE_REC, \n",
    "                                                       INCLUDE_QUALITY, INCLUDE_GENDER, REMOVE_NONE_AUTHORS, EARLY_CAREER, RECOGNITION_CUT)\n",
    "feature_table, feature_table2, feature_table3 = run_elastic_net_cohort(credible_authors, cols_all, cols_std, categorical_cols, EARLY_CAREER)\n",
    "results_no_qual = make_result_table(feature_table)\n",
    "results2_no_qual = make_result_table(feature_table2)\n",
    "results3_no_qual = make_result_table(feature_table3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3_no_qual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_PROD = 1\n",
    "INCLUDE_SOCIAL = 1\n",
    "INCLUDE_REC = 1\n",
    "INCLUDE_GENDER = 0\n",
    "INCLUDE_QUALITY = 1\n",
    "cols_all, cols_std, categorical_cols = make_cols_lists(INCLUDE_PROD, INCLUDE_SOCIAL, INCLUDE_REC,\n",
    "                                                       INCLUDE_QUALITY, INCLUDE_GENDER, REMOVE_NONE_AUTHORS, EARLY_CAREER, RECOGNITION_CUT)\n",
    "feature_table, feature_table2, feature_table3 = run_elastic_net_cohort(credible_authors, cols_all, cols_std, categorical_cols, EARLY_CAREER)\n",
    "results_no_gen = make_result_table(feature_table)\n",
    "results2_no_gen = make_result_table(feature_table2)\n",
    "results3_no_gen = make_result_table(feature_table3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3_no_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_PROD = 1\n",
    "INCLUDE_SOCIAL = 1\n",
    "INCLUDE_REC = 1\n",
    "INCLUDE_GENDER = 0\n",
    "INCLUDE_QUALITY = 0\n",
    "cols_all, cols_std, categorical_cols = make_cols_lists(INCLUDE_PROD, INCLUDE_SOCIAL, INCLUDE_REC,\n",
    "                                                       INCLUDE_QUALITY, INCLUDE_GENDER, REMOVE_NONE_AUTHORS, EARLY_CAREER, RECOGNITION_CUT)\n",
    "feature_table, feature_table2, feature_table3 = run_elastic_net_cohort(credible_authors, cols_all, cols_std, categorical_cols, EARLY_CAREER)\n",
    "results_no_qual_no_gen = make_result_table(feature_table)\n",
    "results2_no_qual_no_gen = make_result_table(feature_table2)\n",
    "results3_no_qual_no_gen = make_result_table(feature_table3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3_no_qual_no_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['r2_no_qual_no_gen'] = results_no_qual_no_gen['r2']\n",
    "results2['r2_no_qual_no_gen'] = results2_no_qual_no_gen['r2']\n",
    "results3['f1_no_qual_no_gen'] = results3_no_qual_no_gen['r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['r2_no_qual'] = results_no_qual['r2']\n",
    "results2['r2_no_qual'] = results2_no_qual['r2']\n",
    "results3['f1_no_qual'] = results3_no_qual['r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['r2_no_gen'] = results_no_gen['r2']\n",
    "results2['r2_no_gen'] = results2_no_gen['r2']\n",
    "results3['f1_no_gen'] = results3_no_gen['r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_percent = credible_authors.groupby('start_year')['dropped_after_10'].sum() / credible_authors.groupby('start_year')['dropped_after_10'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3['drop_percentage'] = dropped_percent.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3 = results3.reindex([results3.columns[0]] + [results3.columns[-1]] + list(results3.columns[1:-1]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.tail()\n",
    "#h_index_increase\n",
    "# train_test results, compare with 'r2' col\n",
    "# Year: 1999, r2: 0.29575585145072425\n",
    "# Year: 2000, r2: 0.3288047075331689\n",
    "# Year: 2001, r2: 0.32359617501275245\n",
    "# Year: 2002, r2: 0.32367401156648834"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2.tail()\n",
    "#citation_increase\n",
    "# train_test results\n",
    "# Year: 1999, r2: 0.26267137837811527\n",
    "# Year: 2000, r2: 0.3229074740899605\n",
    "# Year: 2001, r2: 0.2866603572613974\n",
    "# Year: 2002, r2: 0.30948376141611045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3.tail(4)\n",
    "#dropouts\n",
    "# train_test results\n",
    "# Year: 1999, r2: 0.7871975797159264\n",
    "# Year: 2000, r2: 0.7755906039107061\n",
    "# Year: 2001, r2: 0.7776091854493229\n",
    "# Year: 2002, r2: 0.7783668951214104"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_to_latex(results, 'hindex')\n",
    "results_to_latex(results2, 'cit')\n",
    "results_to_latex(results3, 'dropout')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results\n",
    "# h index increase\n",
    "#results.to_latex()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2\n",
    "# citation increase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3\n",
    "# coefficients are not exponentiated\n",
    "# positive means bigger change to drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test train split 80-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def run_elastic_predictions_test_train(cols_all, cols_std, categorical_cols, EARLY_CAREER):\n",
    "    feature_table = pd.DataFrame(cols_all, columns=['feature']).set_index('feature')\n",
    "    feature_table2 = pd.DataFrame(cols_all, columns=['feature']).set_index('feature')\n",
    "    feature_table3 = pd.DataFrame(cols_all, columns=['feature']).set_index('feature')\n",
    " \n",
    "    for year in [1970,1999,2000,2001,2002]:\n",
    "        credible_authors_year = credible_authors[credible_authors.start_year == year]\n",
    "\n",
    "        X = credible_authors_year.copy()\n",
    "\n",
    "        #remove non-gender rows\n",
    "        if(REMOVE_NONE_AUTHORS):\n",
    "            X = X[X[\"gender\"]!=\"none\"]\n",
    "\n",
    "        # Make dummy values for categorical columns\n",
    "        gender_cols = pd.get_dummies(X[categorical_cols])\n",
    "\n",
    "        #if(not REMOVE_NONE_AUTHORS):\n",
    "            # drop gender none?\n",
    "            # this is removing rows gender_none col\n",
    "            #gender_cols.drop('gender_none', axis=1, inplace=True)\n",
    "\n",
    "        #standardize cols_std\n",
    "        if len(cols_std)>0:\n",
    "            standardized_cols = RobustScaler().fit_transform(X[cols_std])\n",
    "\n",
    "\n",
    "        # claudia: here we could do a 20:80 split and save 20% for later test\n",
    "\n",
    "        #combine\n",
    "        H = pd.DataFrame(standardized_cols, index=X.index, columns=cols_std)\n",
    "        if(INCLUDE_GENDER):\n",
    "            H = H.join(gender_cols)\n",
    "\n",
    "        y = X[f'h_index_increase_15_{EARLY_CAREER}']\n",
    "        y2 = X[f'citation_increase_15_{EARLY_CAREER}']\n",
    "        y3 = X['dropped_after_10'].astype(int)\n",
    "        \n",
    "        f1_dropout_list=[]\n",
    "        r2_hindex_list=[]\n",
    "        for i in range(10):\n",
    "            #dropouts\n",
    "            X_train, X_test, y_train, y_test = train_test_split(H, y3, test_size=0.2)\n",
    "            rgs = LogisticRegressionCV(cv=3) #, penalty='l2', solver='liblinear'\n",
    "            #rgs = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
    "            #           early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
    "            #           l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,\n",
    "            #           n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
    "            #           power_t=0.5, random_state=None, shuffle=True, tol=0.001,\n",
    "            #           validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "        \n",
    "            rgs.fit(X_train, y_train)\n",
    "            f1_dropout = f1_score(y_test, rgs.predict(X_test))\n",
    "            f1_dropout_list.append(f1_dropout)\n",
    "            \n",
    "            #h-index increase\n",
    "            X_train, X_test, y_train, y_test = train_test_split(H, y, test_size=0.2)\n",
    "            rgs = ElasticNetCV(cv=3)\n",
    "            #rgs = ElasticNetCV(cv=3, random_state=1000, max_iter=10000,\n",
    "            #       alphas=[1.0], l1_ratio=0.5)\n",
    "            rgs.fit(X_train, y_train)\n",
    "            r2_hindex = r2_score(y_test, rgs.predict(X_test))\n",
    "            print(rgs.alpha_)\n",
    "            r2_hindex_list.append(r2_hindex)\n",
    "            \n",
    "        print(f\"Year: {year}, f1_dropout: {np.mean(f1_dropout_list)}\")\n",
    "        print(f\"Year: {year}, r2_hindex: {np.mean(r2_hindex_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_PROD = 1\n",
    "INCLUDE_SOCIAL = 1\n",
    "INCLUDE_REC = 1\n",
    "INCLUDE_GENDER = 1\n",
    "INCLUDE_QUALITY = 1\n",
    "cols_all, cols_std, categorical_cols = make_cols_lists(INCLUDE_PROD, INCLUDE_SOCIAL, INCLUDE_REC,\n",
    "                                                       INCLUDE_QUALITY, INCLUDE_GENDER, REMOVE_NONE_AUTHORS, EARLY_CAREER, RECOGNITION_CUT)\n",
    "run_elastic_predictions_test_train(cols_all, cols_std, categorical_cols, EARLY_CAREER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test predictive power over different number of observed years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INCLUDE_PROD = 1\n",
    "INCLUDE_SOCIAL = 1\n",
    "INCLUDE_REC = 1\n",
    "INCLUDE_GENDER = 1\n",
    "INCLUDE_QUALITY = 0\n",
    "\n",
    "EARLY_CAREER_LEN_LIST_EXT = [3,5,7,9,11,12]\n",
    "RECOGNITION_CUT_OFF_LIST_EXT = [3,5,7,9,11,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_authors.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_increase = pd.DataFrame(index=years)\n",
    "for EARLY_CAREER, RECOGNITION_CUT in zip(EARLY_CAREER_LEN_LIST_EXT, RECOGNITION_CUT_OFF_LIST_EXT):\n",
    "    cols_all, cols_std, categorical_cols = make_cols_lists(INCLUDE_PROD, INCLUDE_SOCIAL, INCLUDE_REC, INCLUDE_QUALITY, INCLUDE_GENDER, \n",
    "                                         REMOVE_NONE_AUTHORS, EARLY_CAREER, RECOGNITION_CUT)\n",
    "    feature_table, feature_table2, feature_table3 = run_elastic_net_cohort(credible_authors, cols_all, cols_std, categorical_cols, EARLY_CAREER)\n",
    "    results= make_result_table(feature_table)\n",
    "    results2 = make_result_table(feature_table2)\n",
    "    results3 = make_result_table(feature_table3)\n",
    "    r2_increase[f'h_ind_{EARLY_CAREER}'] = results['r2']\n",
    "    r2_increase[f'cit_{EARLY_CAREER}'] = results2['r2']\n",
    "    r2_increase[f'drop_{EARLY_CAREER}'] = results3['r2']\n",
    "    print(f\"Year: {EARLY_CAREER}\")\n",
    "    print(cols_all)\n",
    "    print(r2_increase[f'h_ind_{EARLY_CAREER}'][2002])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_hind_increase = r2_increase[[f\"h_ind_{x}\" for x in EARLY_CAREER_LEN_LIST]]\n",
    "r2_cit_increase = r2_increase[[f\"cit_{x}\" for x in EARLY_CAREER_LEN_LIST]]\n",
    "r2_drop_increase = r2_increase[[f\"drop_{x}\" for x in EARLY_CAREER_LEN_LIST]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_hind_increase.columns = ['3','5','7','9','11','12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt = plot.init_plotting()\n",
    "fig2 = plt.figure()\n",
    "fig2.patch.set_facecolor('white')\n",
    "ax2 = fig2.add_subplot(1,1,1) #, axisbg=\"white\"\n",
    "colors = ('#DE4C2C', '#3BD64C', '#3B9ED6', '#B73BD6', '#F39C12', '#FFC0CB', '#27AE60', '#48C9B0', '#071019') #'#AAB7B8',\n",
    "markers = []\n",
    "for m in Line2D.markers:\n",
    "    try:\n",
    "        if m != ' ' and m != '':\n",
    "            markers.append(m)\n",
    "    except TypeError:\n",
    "        print(\"Typeerror occured\")\n",
    "        pass\n",
    "p=0\n",
    "for row in r2_hind_increase.index:\n",
    "\n",
    "    cohort = r2_hind_increase.loc[year]\n",
    "    \n",
    "    if row % 5 == 0:\n",
    "        ax2.plot(r2_hind_increase.loc[row], label=row, color=colors[p],\n",
    "                     marker=markers[p], markersize=10)\n",
    "        p = p+1 \n",
    "    else:\n",
    "        ax2.plot(r2_hind_increase.loc[row].values, label=None ,color='grey', alpha=0.5)\n",
    "\n",
    "plt.title(\"Predicting h-index increase\")\n",
    "ax2.set_ylabel(\"R sqaured\", fontweight='bold')\n",
    "ax2.set_xlabel('Number of years observed', fontweight='bold')\n",
    "\n",
    "ax2.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_hind_increase\n",
    "\n",
    "for row in r2_hind_increase.index:\n",
    "    plt.plot(r2_hind_increase.loc[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_cit_increase.T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_drop_increase.T.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### predictor diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "h_index = feature_table.transpose().copy()\n",
    "citations = feature_table2.transpose().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(h_index['r2'], label='Increase H index')\n",
    "plt.plot(citations['r2'], label='Increase Citations')\n",
    "print(\"Average difference in r squared\", sum(citations['r2']-h_index['r2'])/len(h_index['r2']))\n",
    "# quality was used as a feature!\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### gender diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# gender coefs\n",
    "plt.plot(results3['gender_m'], label=\"Male\")\n",
    "plt.plot(results3['gender_f'], label=\"Female\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "plt.plot(results3['gender_m'] - results3['gender_f'], label=\"Male-Female diff\")\n",
    "plt.plot(results.index ,np.zeros(len(results)))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### cohort size diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "# ax1.plot(results['r2'], label='r2')\n",
    "ax1.plot(results['adj_r2'], label='adjusted r2', color='C2')\n",
    "ax1.set_ylim([0,1])\n",
    "ax1.set_xlabel('Years')\n",
    "ax1.set_ylabel('R squared', color='C2')\n",
    "ax1.legend(loc=2)\n",
    "\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(results['cohort_size'], label='Cohort size', color='C3')\n",
    "ax2.set_ylabel('Cohort size', color='C3')\n",
    "ax2.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### cheating diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "no_cheating = feature_table2.transpose().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cheat_RC5 = feature_table2.transpose().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "cheat_quality = feature_table2.transpose().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# How does removing the quality affect the r squared?\n",
    "plt.plot(with_quality['adj_r2'], label='With quality')\n",
    "plt.plot(cheat_RC5['adj_r2'], label='With recognition year 5')\n",
    "plt.plot(no_cheating['adj_r2'], label='No cheating')\n",
    "print(\"Average difference in r squared\", sum(with_quality['adj_r2']-no_cheating['adj_r2'])/len(cheat_quality))\n",
    "print(np.mean)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "avg = sum(cheat_quality['adj_r2']-no_cheating['adj_r2'])/len(no_cheating)\n",
    "plt.plot(cheat_quality['adj_r2']-no_cheating['adj_r2'], label='Difference')\n",
    "plt.plot(no_cheating.index, [avg]*len(no_cheating), label='Average diff')\n",
    "plt.title(\"Difference between quality(15y) and recognition(3y)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### scaler diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "std_scaler = feature_table2.transpose().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "rob_scaler = feature_table2.transpose().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# How does changing the scaler affect the r squared?\n",
    "plt.plot(std_scaler['adj_r2'], label='Std')\n",
    "plt.plot(rob_scaler['adj_r2'], label='Rob')\n",
    "print(\"Average difference in r squared\", sum(std_scaler['adj_r2']-rob_scaler['adj_r2'])/len(rob_scaler))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# almost no difference\n",
    "\n",
    "avg = sum(std_scaler['adj_r2']-rob_scaler['adj_r2'])/len(std_scaler)\n",
    "plt.plot(std_scaler['adj_r2']-rob_scaler['adj_r2'], label='Difference')\n",
    "plt.plot(std_scaler.index, [avg]*len(std_scaler), label='Average diff')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# feature_table3.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Best feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFE, RFECV\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def show_kbest(data_frame, target, linear_rel=True, k=4):\n",
    "    \"\"\"\n",
    "    Selecting K-Best features for classification\n",
    "    :param data_frame: A pandas dataFrame with the training data\n",
    "    :param target: target variable name in DataFrame\n",
    "    :param k: desired number of features from the data\n",
    "    :returns feature_scores: scores for each feature in the data as \n",
    "    pandas DataFrame\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    if linear_rel == True: \n",
    "        feat_selector = SelectKBest(f_regression, k=k)\n",
    "        col_name = \"F Score\"\n",
    "    else:\n",
    "        feat_selector = SelectKBest(mutual_info_regression, k=k)\n",
    "        col_name = \"Mutual Information\"\n",
    "    \n",
    "    feat_selector = feat_selector.fit(data_frame, target)\n",
    "    feat_scores = pd.DataFrame()\n",
    "    feat_scores[col_name] = feat_selector.scores_\n",
    "    feat_scores[\"P Value\"] = feat_selector.pvalues_\n",
    "    feat_scores[\"Support\"] = feat_selector.get_support()\n",
    "    feat_scores[\"Attribute\"] = data_frame.columns\n",
    "    \n",
    "    return feat_scores \n",
    "\n",
    "def get_features_rfe(data_frame, target, model,k=5):\n",
    "    \"\"\"\n",
    "    Returns list of features (k specified) selected using RFE for\n",
    "    :param data_frame: A pandas dataFrame with features and labels\n",
    "    :param k: top k features to select  \n",
    "    :returns list: most relevant features \n",
    "    \"\"\"\n",
    "    X = data_frame\n",
    "    y = target\n",
    "    selector = RFE(model, k, step=1)\n",
    "    selector = selector.fit(X, y)\n",
    "#     print(selector.support_)\n",
    "    df = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"support\": selector.support_\n",
    "    })\n",
    "    return df\n",
    "\n",
    "def get_features_rfecv(data_frame, target, model, cv=3):\n",
    "    \"\"\"\n",
    "    Returns list of features (k specified) selected using RFE for\n",
    "    :param data_frame: A pandas dataFrame with features and labels\n",
    "    :param k: top k features to select  \n",
    "    :returns list: most relevant features \n",
    "    \"\"\"\n",
    "    X = data_frame\n",
    "    y = target\n",
    "    selector = RFECV(model, step=1, cv=cv)\n",
    "    selector = selector.fit(X, y)\n",
    "#     print(selector.support_)\n",
    "    df = pd.DataFrame({\n",
    "        \"feature\": X.columns,\n",
    "        \"support\": selector.support_\n",
    "    })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "years = credible_authors.start_year.unique()\n",
    "years = sorted(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df = credible_authors.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "categorical_cols = ['gender']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df = df.join(pd.get_dummies(df[categorical_cols]))\n",
    "\n",
    "df.drop(categorical_cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Show k best - F regression or mutual information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "linear = True\n",
    "# true - fregression\n",
    "# false - mutual info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params = []\n",
    "for year in years:\n",
    "    df_year = df[df.start_year == year]\n",
    "    df_year = df_year.drop('start_year', axis=1)\n",
    "#     for EARLY_CAREER in EARLY_CAREER_LEN_LIST:\n",
    "#         for RECOGNITION_CUT in RECOGNITION_CUT_OFF_LIST:\n",
    "#             if RECOGNITION_CUT < EARLY_CAREER: continue\n",
    "    EARLY_CAREER = 3\n",
    "    RECOGNITION_CUT= 5\n",
    "    X_year = df_year[['max_absence-0-3', 'avg_absence-0-3', \n",
    "           'gender_f', 'gender_m', 'gender_none',\n",
    "           f'early_career_degree_{EARLY_CAREER}', \n",
    "           f'early_career_prod_{EARLY_CAREER}',\n",
    "           f'early_career_qual_{EARLY_CAREER}', f'early_career_recognition_EC{EARLY_CAREER}_RC{RECOGNITION_CUT}']]\n",
    "    y_year = df_year['succ_after_15y']\n",
    "    params.append(show_kbest(X_year, y_year, linear, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selected_f = defaultdict(int)\n",
    "for param in params:\n",
    "    selected = param[param.Support == True]['Attribute'].values\n",
    "    selected_f['total'] += 1\n",
    "#     print(selected)\n",
    "    for select in selected:\n",
    "        selected_f[select] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selected_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params_rfe = []\n",
    "for year in years:\n",
    "    df_year = df[df.start_year == year]\n",
    "    df_year = df_year.drop('start_year', axis=1)\n",
    "#     for EARLY_CAREER in EARLY_CAREER_LEN_LIST:\n",
    "#         for RECOGNITION_CUT in RECOGNITION_CUT_OFF_LIST:\n",
    "#             if RECOGNITION_CUT < EARLY_CAREER: continue\n",
    "    EARLY_CAREER = 3\n",
    "    RECOGNITION_CUT= 5\n",
    "    X_year = df_year[[\n",
    "        #'max_absence-0-3', 'avg_absence-0-3', \n",
    "           'gender_f', 'gender_m', 'gender_none',\n",
    "           f'early_career_degree_{EARLY_CAREER}', \n",
    "           f'early_career_prod_{EARLY_CAREER}',\n",
    "           f'early_career_qual_{EARLY_CAREER}', f'early_career_recognition_EC{EARLY_CAREER}_RC{RECOGNITION_CUT}']]\n",
    "    y_year = df_year['succ_after_15y']\n",
    "    params_rfe.append(get_features_rfe(X_year, y_year, LinearRegression(),k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selected_f = defaultdict(int)\n",
    "for param in params_rfe:\n",
    "    selected = param[param.support == True]['feature'].values\n",
    "    selected_f['total'] += 1\n",
    "#     print(selected)\n",
    "    for select in selected:\n",
    "        selected_f[select] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selected_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### RFE CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "params_rfecv = []\n",
    "for year in years:\n",
    "    df_year = df[df.start_year == year]\n",
    "    df_year = df_year.drop('start_year', axis=1)\n",
    "#     for EARLY_CAREER in EARLY_CAREER_LEN_LIST:\n",
    "#         for RECOGNITION_CUT in RECOGNITION_CUT_OFF_LIST:\n",
    "#             if RECOGNITION_CUT < EARLY_CAREER: continue\n",
    "    EARLY_CAREER = 3\n",
    "    RECOGNITION_CUT= 5\n",
    "    X_year = df_year[['max_absence-0-3', 'avg_absence-0-3', \n",
    "           'gender_f', 'gender_m', 'gender_none',\n",
    "           f'early_career_degree_{EARLY_CAREER}', \n",
    "           f'early_career_prod_{EARLY_CAREER}',\n",
    "           f'early_career_qual_{EARLY_CAREER}', f'early_career_recognition_EC{EARLY_CAREER}_RC{RECOGNITION_CUT}']]\n",
    "    y_year = df_year['succ_after_15y']\n",
    "    params_rfecv.append(get_features_rfecv(X_year, y_year, LinearRegression(),cv=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selected_f = defaultdict(int)\n",
    "for param in params_rfecv:\n",
    "    selected = param[param.support == True]['feature'].values\n",
    "    selected_f['total'] += 1\n",
    "#     print(selected)\n",
    "    for select in selected:\n",
    "        selected_f[select] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "selected_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "citations_per_year = pd.read_csv('derived-data/paper-citation-count.csv', header=None, names=['pub_id', 'cit_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = pd.read_csv('derived-data/author-publications.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publications.sort_values(by='author').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove authors by career_len, and add start year\n",
    "publications = publications.merge(credible_authors[['author', 'start_year']], on='author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = publications[publications.year <= publications.year + MAX_CAREER_LEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# citations_per_year.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications['pub_id'] = shuffle(publications['pub_id']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publications.sort_values(by='author').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = publications.merge(citations_per_year, on='pub_id', how='left')\n",
    "publications = publications.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications.sort_values(by='author').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_authors[credible_authors.author == \"a min tjoa\"]['succ_after_15y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_authors.set_index('author', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_authors['succ_shuffled'] = publications.groupby('author')['cit_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_authors[['succ_shuffled', 'succ_after_15y']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_authors.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
